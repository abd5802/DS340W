{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#!pip install tensorflow"
      ],
      "metadata": {
        "id": "jyCNs9NTxTSF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Set up environment\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer as tf\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "import csv\n",
        "import pickle\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, regularizers\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "6DllGd2gn6nb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "results = []"
      ],
      "metadata": {
        "id": "63fEjyIcr_vV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to load and preprocess TSV data\n",
        "def load_data(file_path):\n",
        "    # Load the TSV file\n",
        "    data = pd.read_csv(file_path, header=None)\n",
        "    # Extract the statement and label\n",
        "    texts = data[2].tolist()\n",
        "    # Map labels to binary (1 for false/pants-fire, 0 for other)\n",
        "    labels = data[1].apply(lambda x: 1 if x in ['false', 'pants-fire'] else 0).tolist()\n",
        "\n",
        "    return texts, labels\n",
        "\n",
        "# Load training, test, and validation sets\n",
        "train_texts, train_labels = load_data('LiarTrain.csv')\n",
        "test_texts, test_labels = load_data('LiarTest.csv')\n",
        "valid_texts, valid_labels = load_data('LiarValid.csv')\n",
        "\n",
        "# Combine train and validation data\n",
        "texts = train_texts + valid_texts\n",
        "labels = train_labels + valid_labels\n",
        "\n",
        "# Split into training and testing sets\n",
        "x_train, x_test, y_train, y_test = train_texts, test_texts, train_labels, test_labels\n",
        "\n",
        "# Vectorize texts using TF-IDF with bigrams\n",
        "tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2), stop_words='english')\n",
        "x_train_tfidf = tfidf.fit_transform(x_train)\n",
        "x_test_tfidf = tfidf.transform(x_test)"
      ],
      "metadata": {
        "id": "P7m4SwZ3xmfi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Naive Bayes"
      ],
      "metadata": {
        "id": "uJKXfihYuG7X"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Naive Bayes Model\n",
        "clf_nb = MultinomialNB()\n",
        "print(\"Training Naive Bayes classifier...\")\n",
        "clf_nb.fit(x_train_tfidf, y_train)\n",
        "nb_pred = clf_nb.predict(x_test_tfidf)\n",
        "\n",
        "nb_accuracy = accuracy_score(y_test, nb_pred)\n",
        "nb_precision = precision_score(y_test, nb_pred, average=\"weighted\")\n",
        "nb_recall = recall_score(y_test, nb_pred, average=\"weighted\")\n",
        "nb_f1 = f1_score(y_test, nb_pred, average=\"weighted\")\n",
        "\n",
        "results.append([\"Naive Bayes\", nb_accuracy, nb_precision, nb_recall, nb_f1])"
      ],
      "metadata": {
        "id": "J5JXZrCQdi8r",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc83e160-d53f-4db2-f4d9-992202ec73fa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Naive Bayes classifier...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Decision Tree"
      ],
      "metadata": {
        "id": "tB1E2BdKuILs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Decision Tree Model\n",
        "clf_dt = DecisionTreeClassifier(random_state=42)\n",
        "print(\"Training Decision Tree classifier...\")\n",
        "clf_dt.fit(x_train_tfidf, y_train)\n",
        "dt_pred = clf_dt.predict(x_test_tfidf)\n",
        "\n",
        "dt_accuracy = accuracy_score(y_test, dt_pred)\n",
        "dt_precision = precision_score(y_test, dt_pred, average=\"weighted\")\n",
        "dt_recall = recall_score(y_test, dt_pred, average=\"weighted\")\n",
        "dt_f1 = f1_score(y_test, dt_pred, average=\"weighted\")\n",
        "\n",
        "results.append([\"Decision Tree\", dt_accuracy, dt_precision, dt_recall, dt_f1])"
      ],
      "metadata": {
        "id": "tl4LCvxPdivQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d4391a0-95cc-4565-99d5-c13c20a8fcc5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Decision Tree classifier...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SMHA-CNN"
      ],
      "metadata": {
        "id": "_Liz8fI-uEXO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def build_smha_cnn(vocab_size, embedding_dim):\n",
        "    # Input layer\n",
        "    inputs = tf.keras.Input(shape=(100,), dtype=tf.int32)  # Assuming 100 is the max sequence length\n",
        "\n",
        "    # Embedding layer\n",
        "    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)\n",
        "\n",
        "    # CNN Layer\n",
        "    x = layers.Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)\n",
        "    x = layers.MaxPooling1D(pool_size=2)(x)\n",
        "    x = layers.Dropout(0.5)(x)\n",
        "\n",
        "\n",
        "    # Self-Multi-Head Attention Layer\n",
        "    attention = layers.MultiHeadAttention(num_heads=8, key_dim=embedding_dim)\n",
        "\n",
        "    # Tensor reshaping to match attention layer format\n",
        "    x = attention(x, x)\n",
        "\n",
        "    # Global Max Pooling\n",
        "    x = layers.GlobalMaxPooling1D()(x)\n",
        "\n",
        "    # Fully connected layer\n",
        "    x = layers.Dense(64, activation='relu')(x)\n",
        "\n",
        "    # Output layer\n",
        "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "    # Build and compile the model\n",
        "    model = tf.keras.Model(inputs, outputs)\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Tokenize texts for SMHA-CNN\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
        "x_train_seq = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen=100)\n",
        "x_test_seq = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen=100)\n",
        "\n",
        "# Get the vocabulary size for embedding layer\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "\n",
        "# Convert numpy arrays to tensors before passing to the model\n",
        "x_train_seq = tf.convert_to_tensor(x_train_seq, dtype=tf.int32)\n",
        "x_test_seq = tf.convert_to_tensor(x_test_seq, dtype=tf.int32)\n",
        "y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)\n",
        "y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)\n",
        "\n",
        "# Train the SMHA-CNN model\n",
        "print(\"Training SMHA-CNN model...\")\n",
        "smha_cnn = build_smha_cnn(vocab_size, 128)\n",
        "history = smha_cnn.fit(x_train_seq, y_train, epochs=4, batch_size=32, validation_data=(x_test_seq, y_test))\n",
        "cnn_pred = (smha_cnn.predict(x_test_seq) > 0.5).astype(\"int32\").flatten()\n",
        "\n",
        "cnn_accuracy = accuracy_score(y_test, cnn_pred)\n",
        "cnn_precision = precision_score(y_test, cnn_pred)\n",
        "cnn_recall = recall_score(y_test, cnn_pred)\n",
        "cnn_f1 = f1_score(y_test, cnn_pred)\n",
        "\n",
        "results.append([\"SMHA-CNN\", cnn_accuracy, cnn_precision, cnn_recall, cnn_f1])"
      ],
      "metadata": {
        "id": "1r8pxGTXoCeK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c16869f-b51a-4205-fa5c-65a4e4373590"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training SMHA-CNN model...\n",
            "Epoch 1/4\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m123s\u001b[0m 375ms/step - accuracy: 0.7136 - loss: 0.9071 - val_accuracy: 0.7309 - val_loss: 0.5710\n",
            "Epoch 2/4\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m143s\u001b[0m 378ms/step - accuracy: 0.7236 - loss: 0.5674 - val_accuracy: 0.7364 - val_loss: 0.5789\n",
            "Epoch 3/4\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 377ms/step - accuracy: 0.7732 - loss: 0.4796 - val_accuracy: 0.6922 - val_loss: 0.6174\n",
            "Epoch 4/4\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m144s\u001b[0m 385ms/step - accuracy: 0.8248 - loss: 0.4113 - val_accuracy: 0.6772 - val_loss: 0.6378\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 94ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Logistic Regression"
      ],
      "metadata": {
        "id": "pviSA9D6xRMY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Train the Logistic Regression model\n",
        "clf_lr = LogisticRegression(max_iter=1000)  # Increase max_iter if the model doesn't converge\n",
        "print(\"Training Logistic Regression model...\")\n",
        "clf_lr.fit(x_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "lr_pred = clf_lr.predict(x_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "log_accuracy = accuracy_score(y_test, lr_pred)\n",
        "log_precision = precision_score(y_test, lr_pred)\n",
        "log_recall = recall_score(y_test, lr_pred)\n",
        "log_f1 = f1_score(y_test, lr_pred)\n",
        "\n",
        "results.append([\"Logistic Reg\", log_accuracy, log_precision, log_recall, log_f1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2l4zI6fHxMtj",
        "outputId": "8b9955ed-09f8-4caf-c1b9-9419f5c429b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression model...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "XGBoost"
      ],
      "metadata": {
        "id": "zynZHG-d5Dwp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Train the XGBoost classifier\n",
        "clf_xgb = xgb.XGBClassifier(use_label_encoder=False, eval_metric='logloss')\n",
        "clf_xgb.fit(x_train_tfidf, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "xgb_pred = clf_xgb.predict(x_test_tfidf)\n",
        "\n",
        "# Evaluate the model\n",
        "XGBoost_Accuracy = accuracy_score(y_test, xgb_pred)\n",
        "XGBoost_Precision = precision_score(y_test, xgb_pred)\n",
        "XGBoost_Recall = recall_score(y_test, xgb_pred)\n",
        "XGBoost_f1 = f1_score(y_test, xgb_pred)\n",
        "\n",
        "results.append([\"XGBoost\", XGBoost_Accuracy, XGBoost_Precision, XGBoost_Recall, XGBoost_f1])"
      ],
      "metadata": {
        "id": "qd-5ygQQ4oPi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e5f17b2-2220-4b7e-df1a-cf1e0b4f9fc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/xgboost/core.py:158: UserWarning: [02:14:57] WARNING: /workspace/src/learner.cc:740: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CNN"
      ],
      "metadata": {
        "id": "qsBGLlsw5B3k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Tokenizer to convert text to sequences of integers\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
        "tokenizer.fit_on_texts(x_train)\n",
        "\n",
        "# Convert text to sequences\n",
        "x_train_seq = tokenizer.texts_to_sequences(x_train)\n",
        "x_test_seq = tokenizer.texts_to_sequences(x_test)\n",
        "\n",
        "# Pad the sequences to a fixed length\n",
        "x_train_seq = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen=100)\n",
        "x_test_seq = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen=100)\n",
        "\n",
        "# Convert labels to numpy arrays\n",
        "y_train = np.array(y_train)\n",
        "y_test = np.array(y_test)\n",
        "\n",
        "def build_cnn(vocab_size, embedding_dim, input_length=100):\n",
        "    model = models.Sequential()\n",
        "\n",
        "    # Embedding layer\n",
        "    model.add(layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, input_length=input_length))\n",
        "\n",
        "    # Add 1D Convolutional Layers\n",
        "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "    model.add(layers.MaxPooling1D(5))\n",
        "\n",
        "    model.add(layers.Conv1D(128, 5, activation='relu'))\n",
        "    model.add(layers.MaxPooling1D(5))\n",
        "\n",
        "    # Flatten the output to connect it to a fully connected layer\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    # Fully connected layer\n",
        "    model.add(layers.Dense(64, activation='relu'))\n",
        "\n",
        "    # Output layer\n",
        "    model.add(layers.Dense(1, activation='sigmoid'))\n",
        "\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    return model\n",
        "\n",
        "# Get the total number of unique words\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_dim = 128\n",
        "\n",
        "# Build the model\n",
        "cnn_model = build_cnn(vocab_size, embedding_dim)\n",
        "\n",
        "# Train the model\n",
        "cnn_model.fit(x_train_seq, y_train, epochs=2, batch_size=32, validation_data=(x_test_seq, y_test))\n",
        "\n",
        "# Predict the test data\n",
        "cnn_pred = (cnn_model.predict(x_test_seq) > 0.5).astype(\"int32\")\n",
        "\n",
        "# Evaluate model performance\n",
        "CNN_Accuracy = accuracy_score(y_test, cnn_pred)\n",
        "CNN_Precision = precision_score(y_test, cnn_pred)\n",
        "CNN_Recall = recall_score(y_test, cnn_pred)\n",
        "CNN_f1 = f1_score(y_test, cnn_pred)\n",
        "\n",
        "results.append([\"CNN\", CNN_Accuracy, CNN_Precision, CNN_Recall, CNN_f1])\n"
      ],
      "metadata": {
        "id": "84m9j6H54yPl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b0f300a2-365f-4b8d-80f5-3637cd46ac81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/2\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 89ms/step - accuracy: 0.7129 - loss: 0.5927 - val_accuracy: 0.7309 - val_loss: 0.5613\n",
            "Epoch 2/2\n",
            "\u001b[1m320/320\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 88ms/step - accuracy: 0.7548 - loss: 0.4887 - val_accuracy: 0.7167 - val_loss: 0.5962\n",
            "\u001b[1m40/40\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "results_df = pd.DataFrame(results, columns=[\"Model\", \"Accuracy\", \"Precision\", \"Recall\", \"F1-Score\"])\n",
        "print(\"\\nResults Table:\")\n",
        "print(results_df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XnVDkwB7_xa6",
        "outputId": "ae6e6365-2383-4895-aeda-8a184f1ccab7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Results Table:\n",
            "           Model  Accuracy  Precision    Recall  F1-Score\n",
            "0    Naive Bayes  0.730860   0.670612  0.730860  0.627099\n",
            "1  Decision Tree  0.640095   0.641426  0.640095  0.640754\n",
            "2       SMHA-CNN  0.677190   0.357143  0.249267  0.293610\n",
            "3   Logistic Reg  0.731650   0.512195  0.061584  0.109948\n",
            "4        XGBoost  0.726914   0.460317  0.085044  0.143564\n",
            "5            CNN  0.716654   0.456311  0.275660  0.343693\n"
          ]
        }
      ]
    }
  ]
}
