# -*- coding: utf-8 -*-
"""DS340MidtermCode.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1zjdhh61IFHLYMJmIn56zOnlqw9Xlbwlp
"""

!pip install tensorflow

import pandas as pd
from sklearn.feature_extraction.text import TfidfVectorizer as tf
import numpy as np
from sklearn.naive_bayes import MultinomialNB
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import train_test_split
from sklearn.feature_extraction.text import TfidfVectorizer
import tensorflow as tf
from tensorflow.keras import layers, regularizers
import csv
import pickle # check if we need this
import matplotlib.pyplot as plt
from tensorflow.keras import layers
import matplotlib.pyplot as plt



# Function to load and preprocess TSV data
def load_data(file_path):
    # Load the TSV file
    data = pd.read_csv(file_path, header=None)
    # Extract the statement (Column 3) and label (Column 2)
    texts = data[2].tolist()

    # Map labels to binary (1 for false/pants-fire, 0 for other)
    labels = data[1].apply(lambda x: 1 if x in ['false', 'pants-fire'] else 0).tolist()

    return texts, labels

# Load training, test, and validation sets
train_texts, train_labels = load_data('LiarTrain.csv')
test_texts, test_labels = load_data('LiarTest.csv')
valid_texts, valid_labels = load_data('LiarValid.csv')

# Combine train and validation data for training
texts = train_texts + valid_texts
labels = train_labels + valid_labels

# Split into training and testing sets (keeping test data as separate)
x_train, x_test, y_train, y_test = train_texts, test_texts, train_labels, test_labels

# Proceed with text vectorization and model training... (keeping the original flow)
# Vectorize texts using TF-IDF with bigrams
tfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2), stop_words='english')
x_train_tfidf = tfidf.fit_transform(x_train)
x_test_tfidf = tfidf.transform(x_test)

# Naive Bayes Model
clf_nb = MultinomialNB()
print("Training Naive Bayes classifier...")
clf_nb.fit(x_train_tfidf, y_train)
nb_pred = clf_nb.predict(x_test_tfidf)

print("Naive Bayes Accuracy:", accuracy_score(y_test, nb_pred))

# Decision Tree Model
clf_dt = DecisionTreeClassifier(random_state=42)
print("Training Decision Tree classifier...")
clf_dt.fit(x_train_tfidf, y_train)
dt_pred = clf_dt.predict(x_test_tfidf)

print("Decision Tree Accuracy:", accuracy_score(y_test, dt_pred))

def build_smha_cnn(vocab_size, embedding_dim):
    # Input layer (ensure it's a tensor)
    inputs = tf.keras.Input(shape=(100,), dtype=tf.int32)  # Assuming 100 is the max sequence length

    # Embedding layer
    x = layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim)(inputs)

    # CNN Layer
    x = layers.Conv1D(filters=128, kernel_size=5, activation='relu', kernel_regularizer=regularizers.l2(0.01))(x)
    x = layers.MaxPooling1D(pool_size=2)(x)
    x = layers.Dropout(0.5)(x)


    # Self-Multi-Head Attention Layer
    attention = layers.MultiHeadAttention(num_heads=8, key_dim=embedding_dim)

    # Tensor reshaping to match attention layer format
    x = attention(x, x)

    # Global Max Pooling
    x = layers.GlobalMaxPooling1D()(x)

    # Fully connected layer
    x = layers.Dense(64, activation='relu')(x)

    # Output layer (binary classification)
    outputs = layers.Dense(1, activation='sigmoid')(x)

    # Build and compile the model
    model = tf.keras.Model(inputs, outputs)
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    return model

# Tokenize texts for SMHA-CNN
tokenizer = tf.keras.preprocessing.text.Tokenizer()
tokenizer.fit_on_texts(x_train)
x_train_seq = tokenizer.texts_to_sequences(x_train)
x_test_seq = tokenizer.texts_to_sequences(x_test)
x_train_seq = tf.keras.preprocessing.sequence.pad_sequences(x_train_seq, maxlen=100)
x_test_seq = tf.keras.preprocessing.sequence.pad_sequences(x_test_seq, maxlen=100)

# Get the vocabulary size for embedding layer
vocab_size = len(tokenizer.word_index) + 1

# Convert numpy arrays to tensors before passing to the model
x_train_seq = tf.convert_to_tensor(x_train_seq, dtype=tf.int32)
x_test_seq = tf.convert_to_tensor(x_test_seq, dtype=tf.int32)
y_train = tf.convert_to_tensor(y_train, dtype=tf.int32)
y_test = tf.convert_to_tensor(y_test, dtype=tf.int32)

# Train the SMHA-CNN model
smha_cnn = build_smha_cnn(vocab_size, 128)
print("Training SMHA-CNN model...")
history = smha_cnn.fit(x_train_seq, y_train, epochs=5, batch_size=32, validation_data=(x_test_seq, y_test))
cnn_pred = (smha_cnn.predict(x_test_seq) > 0.5).astype("int32").flatten()